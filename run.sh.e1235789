/work/gr41/r41000/openpi/.venv/lib/python3.11/site-packages/tyro/_parsers.py:347: UserWarning: The field `data.repo-id` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
01:35:52.981 [I] Created experiment checkpoint directory: /work/gr41/r41000/openpi/checkpoints/pi0_ur3_robotiq_cartesian/full_trial (423967:train_pytorch.py:355)
wandb: Currently logged in as: kazuki-takahashi-research (kazuki-takahashi-research-university-of-tokyo) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 2tp11v86
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /work/gr41/r41000/openpi/wandb/run-20260101_013553-2tp11v86
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-dew-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kazuki-takahashi-research-university-of-tokyo/pi0_ur3_robotiq_cartesian
wandb: üöÄ View run at https://wandb.ai/kazuki-takahashi-research-university-of-tokyo/pi0_ur3_robotiq_cartesian/runs/2tp11v86
01:35:54.722 [I] Using batch size per GPU: 32 (total batch size across 1 GPUs: 32)                (423967:train_pytorch.py:369)
01:35:54.725 [I] Loaded norm stats from /work/gr41/r41000/openpi/assets/pi0_ur3_robotiq_cartesian/lan_ur3_lerobot_cartesian (423967:config.py:204)
01:35:54.791 [I] data_config: DataConfig(repo_id=None, local_repo_path='/work/gr41/r41000/data/lan_ur3_lerobot_cartesian', asset_id='lan_ur3_lerobot_cartesian', norm_stats={'state': NormStats(mean=array([-2.27635697e-01, -3.21690649e-01,  3.23561847e-01,  3.83360982e-01,
        9.23553824e-01, -5.50533645e-04, -7.37393228e-03,  2.36360490e-01]), std=array([0.05834112, 0.05373375, 0.01759931, 0.00136479, 0.00100662,
       0.00367024, 0.00345118, 0.20053811]), q01=array([-0.34980083, -0.40926028,  0.28143078,  0.3805485 ,  0.92190974,
       -0.0095531 , -0.01568498,  0.02866652]), q99=array([-0.13496986, -0.24075119,  0.3553676 ,  0.38710319,  0.92472905,
        0.00695206, -0.00166041,  0.56617024])), 'actions': NormStats(mean=array([-2.29860440e-01, -3.21134329e-01,  3.23482245e-01,  3.83387774e-01,
        9.23541725e-01, -6.17606740e-04, -7.46462774e-03,  2.19359204e-01]), std=array([0.05877483, 0.05421592, 0.0176532 , 0.00139182, 0.00109183,
       0.00362014, 0.00343672, 0.20768645]), q01=array([-3.50328138e-01, -4.09368801e-01,  2.81435799e-01,  3.80554487e-01,
        9.21888957e-01, -9.52660770e-03, -1.56668148e-02,  6.88368862e-10]), q99=array([-0.13599704, -0.24022586,  0.35535109,  0.38714826,  0.92473075,
        0.00692012, -0.00176116,  0.56510433]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'images.cam_fixed', 'cam_left_wrist': 'images.cam_wrist'}, 'state': 'state', 'actions': 'actions', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=[Ur3RobotiqInputs(model_type=<ModelType.PI0: 'pi0'>)], outputs=[Ur3RobotiqOutputs(action_dims=7)]), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x400232caaf90>, discrete_state_input=False), PadStatesAndActions(model_action_dim=8)], outputs=()), use_quantile_norm=False, action_sequence_keys=(), prompt_from_task=True, rlds_data_dir=None, action_space=None, datasets=(), train_fraction=0.9, split_seed=0) (423967:data_loader.py:260)
01:35:54.861 [W] 'torchcodec' is not available in your platform, falling back to 'pyav' as a default decoder (423967:video_utils.py:37)
01:35:56.368 [I] local_batch_size: 32                                                             (423967:data_loader.py:356)
INFO:2026-01-01 01:35:56,740:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
01:35:56.740 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (423967:xla_bridge.py:925)
INFO:2026-01-01 01:35:56,749:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
01:35:56.749 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (423967:xla_bridge.py:925)
01:35:56.751 [I] Loaded norm stats from /work/gr41/r41000/openpi/assets/pi0_ur3_robotiq_cartesian/lan_ur3_lerobot_cartesian (423967:config.py:204)
01:35:56.816 [I] data_config: DataConfig(repo_id=None, local_repo_path='/work/gr41/r41000/data/lan_ur3_lerobot_cartesian', asset_id='lan_ur3_lerobot_cartesian', norm_stats={'state': NormStats(mean=array([-2.27635697e-01, -3.21690649e-01,  3.23561847e-01,  3.83360982e-01,
        9.23553824e-01, -5.50533645e-04, -7.37393228e-03,  2.36360490e-01]), std=array([0.05834112, 0.05373375, 0.01759931, 0.00136479, 0.00100662,
       0.00367024, 0.00345118, 0.20053811]), q01=array([-0.34980083, -0.40926028,  0.28143078,  0.3805485 ,  0.92190974,
       -0.0095531 , -0.01568498,  0.02866652]), q99=array([-0.13496986, -0.24075119,  0.3553676 ,  0.38710319,  0.92472905,
        0.00695206, -0.00166041,  0.56617024])), 'actions': NormStats(mean=array([-2.29860440e-01, -3.21134329e-01,  3.23482245e-01,  3.83387774e-01,
        9.23541725e-01, -6.17606740e-04, -7.46462774e-03,  2.19359204e-01]), std=array([0.05877483, 0.05421592, 0.0176532 , 0.00139182, 0.00109183,
       0.00362014, 0.00343672, 0.20768645]), q01=array([-3.50328138e-01, -4.09368801e-01,  2.81435799e-01,  3.80554487e-01,
        9.21888957e-01, -9.52660770e-03, -1.56668148e-02,  6.88368862e-10]), q99=array([-0.13599704, -0.24022586,  0.35535109,  0.38714826,  0.92473075,
        0.00692012, -0.00176116,  0.56510433]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'images.cam_fixed', 'cam_left_wrist': 'images.cam_wrist'}, 'state': 'state', 'actions': 'actions', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=[Ur3RobotiqInputs(model_type=<ModelType.PI0: 'pi0'>)], outputs=[Ur3RobotiqOutputs(action_dims=7)]), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x400233d64a50>, discrete_state_input=False), PadStatesAndActions(model_action_dim=8)], outputs=()), use_quantile_norm=False, action_sequence_keys=(), prompt_from_task=True, rlds_data_dir=None, action_space=None, datasets=(), train_fraction=0.9, split_seed=0) (423967:data_loader.py:260)
01:35:57.006 [W] 'torchcodec' is not available in your platform, falling back to 'pyav' as a default decoder (423967:video_utils.py:37)
01:35:57.973 [I] local_batch_size: 32                                                             (423967:data_loader.py:356)
01:35:57.975 [I] Loaded norm stats from /work/gr41/r41000/openpi/assets/pi0_ur3_robotiq_cartesian/lan_ur3_lerobot_cartesian (423967:config.py:204)
01:35:58.031 [I] data_config: DataConfig(repo_id=None, local_repo_path='/work/gr41/r41000/data/lan_ur3_lerobot_cartesian', asset_id='lan_ur3_lerobot_cartesian', norm_stats={'state': NormStats(mean=array([-2.27635697e-01, -3.21690649e-01,  3.23561847e-01,  3.83360982e-01,
        9.23553824e-01, -5.50533645e-04, -7.37393228e-03,  2.36360490e-01]), std=array([0.05834112, 0.05373375, 0.01759931, 0.00136479, 0.00100662,
       0.00367024, 0.00345118, 0.20053811]), q01=array([-0.34980083, -0.40926028,  0.28143078,  0.3805485 ,  0.92190974,
       -0.0095531 , -0.01568498,  0.02866652]), q99=array([-0.13496986, -0.24075119,  0.3553676 ,  0.38710319,  0.92472905,
        0.00695206, -0.00166041,  0.56617024])), 'actions': NormStats(mean=array([-2.29860440e-01, -3.21134329e-01,  3.23482245e-01,  3.83387774e-01,
        9.23541725e-01, -6.17606740e-04, -7.46462774e-03,  2.19359204e-01]), std=array([0.05877483, 0.05421592, 0.0176532 , 0.00139182, 0.00109183,
       0.00362014, 0.00343672, 0.20768645]), q01=array([-3.50328138e-01, -4.09368801e-01,  2.81435799e-01,  3.80554487e-01,
        9.21888957e-01, -9.52660770e-03, -1.56668148e-02,  6.88368862e-10]), q99=array([-0.13599704, -0.24022586,  0.35535109,  0.38714826,  0.92473075,
        0.00692012, -0.00176116,  0.56510433]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'images.cam_fixed', 'cam_left_wrist': 'images.cam_wrist'}, 'state': 'state', 'actions': 'actions', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=[Ur3RobotiqInputs(model_type=<ModelType.PI0: 'pi0'>)], outputs=[Ur3RobotiqOutputs(action_dims=7)]), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x400233065510>, discrete_state_input=False), PadStatesAndActions(model_action_dim=8)], outputs=()), use_quantile_norm=False, action_sequence_keys=(), prompt_from_task=True, rlds_data_dir=None, action_space=None, datasets=(), train_fraction=0.9, split_seed=0) (423967:data_loader.py:260)
01:35:58.091 [W] 'torchcodec' is not available in your platform, falling back to 'pyav' as a default decoder (423967:video_utils.py:37)
01:35:59.033 [I] local_batch_size: 32                                                             (423967:data_loader.py:356)
01:36:18.836 [I] Cleared sample batch and data loader from memory                                 (423967:train_pytorch.py:428)
01:37:04.527 [I] Enabled gradient checkpointing for PI0Pytorch model                              (423967:pi0_pytorch.py:134)
01:37:04.527 [I] Enabled gradient checkpointing for memory optimization                           (423967:train_pytorch.py:452)
01:37:04.528 [I] Step 0 (after_model_creation): GPU memory - allocated: 7.02GB, reserved: 7.09GB, free: 0.07GB, peak_allocated: 7.02GB, peak_reserved: 7.09GB (423967:train_pytorch.py:319)
01:37:04.528 [I] Loading weights from: /work/gr41/r41000/.cache/openpi/openpi-assets/checkpoints/pi0_base_pytorch/ (423967:train_pytorch.py:481)
Traceback (most recent call last):
  File "/work/gr41/r41000/openpi/./scripts/train_pytorch.py", line 717, in <module>
    main()
  File "/work/gr41/r41000/openpi/./scripts/train_pytorch.py", line 713, in main
    train_loop(config)
  File "/work/gr41/r41000/openpi/./scripts/train_pytorch.py", line 484, in train_loop
    safetensors.torch.load_model(
  File "/work/gr41/r41000/openpi/.venv/lib/python3.11/site-packages/safetensors/torch.py", line 205, in load_model
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/gr41/r41000/openpi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PI0Pytorch:
	size mismatch for action_in_proj.weight: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([1024, 8]).
	size mismatch for action_out_proj.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([8, 1024]).
	size mismatch for action_out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).
	size mismatch for state_proj.weight: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([1024, 8]).
